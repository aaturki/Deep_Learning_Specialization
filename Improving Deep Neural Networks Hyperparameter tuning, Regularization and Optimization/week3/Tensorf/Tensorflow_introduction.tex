\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Tensorflow\_introduction}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{introduction-to-tensorflow}{%
\section{Introduction to TensorFlow}\label{introduction-to-tensorflow}}

Welcome to this week's programming assignment! Up until now, you've
always used Numpy to build neural networks, but this week you'll explore
a deep learning framework that allows you to build neural networks more
easily. Machine learning frameworks like TensorFlow, PaddlePaddle,
Torch, Caffe, Keras, and many others can speed up your machine learning
development significantly. TensorFlow 2.3 has made significant
improvements over its predecessor, some of which you'll encounter and
implement here!

By the end of this assignment, you'll be able to do the following in
TensorFlow 2.3:

\begin{itemize}
\tightlist
\item
  Use \texttt{tf.Variable} to modify the state of a variable
\item
  Explain the difference between a variable and a constant
\item
  Train a Neural Network on a TensorFlow dataset
\end{itemize}

Programming frameworks like TensorFlow not only cut down on time spent
coding, but can also perform optimizations that speed up the code
itself.

\hypertarget{important-note-on-submission-to-the-autograder}{%
\subsection{Important Note on Submission to the
AutoGrader}\label{important-note-on-submission-to-the-autograder}}

Before submitting your assignment to the AutoGrader, please make sure
you are not doing the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You have not added any \emph{extra} \texttt{print} statement(s) in the
  assignment.
\item
  You have not added any \emph{extra} code cell(s) in the assignment.
\item
  You have not changed any of the function parameters.
\item
  You are not using any global variables inside your graded exercises.
  Unless specifically instructed to do so, please refrain from it and
  use the local variables instead.
\item
  You are not changing the assignment code where it is not required,
  like creating \emph{extra} variables.
\end{enumerate}

If you do any of the following, you will get something like,
\texttt{Grader\ not\ found} (or similarly unexpected) error upon
submitting your assignment. Before asking for help/debugging the errors
in your assignment, check for these first. If this is the case, and you
don't remember the changes you have made, you can get a fresh copy of
the assignment by following these
\href{https://www.coursera.org/learn/deep-neural-network/supplement/QWEnZ/h-ow-to-refresh-your-workspace}{instructions}.

    \hypertarget{table-of-contents}{%
\subsection{Table of Contents}\label{table-of-contents}}

\begin{itemize}
\tightlist
\item
  Section \ref{1}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{1-1}
  \end{itemize}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{2-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-1}
    \end{itemize}
  \item
    Section \ref{2-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-2}
    \end{itemize}
  \item
    Section \ref{2-3}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-3}
    \end{itemize}
  \item
    Section \ref{2-4}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-4}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{3-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-5}
    \end{itemize}
  \item
    Section \ref{3-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-6}
    \end{itemize}
  \item
    Section \ref{3-3}
  \end{itemize}
\item
  Section \ref{4}
\end{itemize}

    \#\# 1 - Packages

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{h5py}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{python}\PY{n+nn}{.}\PY{n+nn}{framework}\PY{n+nn}{.}\PY{n+nn}{ops} \PY{k+kn}{import} \PY{n}{EagerTensor}
\PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{python}\PY{n+nn}{.}\PY{n+nn}{ops}\PY{n+nn}{.}\PY{n+nn}{resource\PYZus{}variable\PYZus{}ops} \PY{k+kn}{import} \PY{n}{ResourceVariable}
\PY{k+kn}{import} \PY{n+nn}{time}
\end{Verbatim}
\end{tcolorbox}

    \#\#\# 1.1 - Checking TensorFlow Version

You will be using v2.3 for this assignment, for maximum speed and
efficiency.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
'2.3.0'
\end{Verbatim}
\end{tcolorbox}
        
    \#\# 2 - Basic Optimization with GradientTape

The beauty of TensorFlow 2 is in its simplicity. Basically, all you need
to do is implement forward propagation through a computational graph.
TensorFlow will compute the derivatives for you, by moving backwards
through the graph recorded with \texttt{GradientTape}. All that's left
for you to do then is specify the cost function and optimizer you want
to use!

When writing a TensorFlow program, the main object to get used and
transformed is the \texttt{tf.Tensor}. These tensors are the TensorFlow
equivalent of Numpy arrays, i.e.~multidimensional arrays of a given data
type that also contain information about the computational graph.

Below, you'll use \texttt{tf.Variable} to store the state of your
variables. Variables can only be created once as its initial value
defines the variable shape and type. Additionally, the \texttt{dtype}
arg in \texttt{tf.Variable} can be set to allow data to be converted to
that type. But if none is specified, either the datatype will be kept if
the initial value is a Tensor, or \texttt{convert\_to\_tensor} will
decide. It's generally best for you to specify directly, so nothing
breaks!

    Here you'll call the TensorFlow dataset created on a HDF5 file, which
you can use in place of a Numpy array to store your datasets. You can
think of this as a TensorFlow data generator!

You will use the Hand sign data set, that is composed of images with
shape 64x64x3.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{h5py}\PY{o}{.}\PY{n}{File}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{datasets/train\PYZus{}signs.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{test\PYZus{}dataset} \PY{o}{=} \PY{n}{h5py}\PY{o}{.}\PY{n}{File}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{datasets/test\PYZus{}signs.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{o}{.}\PY{n}{from\PYZus{}tensor\PYZus{}slices}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}set\PYZus{}x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{o}{.}\PY{n}{from\PYZus{}tensor\PYZus{}slices}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}set\PYZus{}y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{x\PYZus{}test} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{o}{.}\PY{n}{from\PYZus{}tensor\PYZus{}slices}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}set\PYZus{}x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{o}{.}\PY{n}{from\PYZus{}tensor\PYZus{}slices}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}set\PYZus{}y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{type}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
tensorflow.python.data.ops.dataset\_ops.TensorSliceDataset
\end{Verbatim}
\end{tcolorbox}
        
    Since TensorFlow Datasets are generators, you can't access directly the
contents unless you iterate over them in a for loop, or by explicitly
creating a Python iterator using \texttt{iter} and consuming its
elements using \texttt{next}. Also, you can inspect the \texttt{shape}
and \texttt{dtype} of each element using the \texttt{element\_spec}
attribute.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{element\PYZus{}spec}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TensorSpec(shape=(64, 64, 3), dtype=tf.uint8, name=None)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{next}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor(
[[[227 220 214]
  [227 221 215]
  [227 222 215]
  {\ldots}
  [232 230 224]
  [231 229 222]
  [230 229 221]]

 [[227 221 214]
  [227 221 215]
  [228 221 215]
  {\ldots}
  [232 230 224]
  [231 229 222]
  [231 229 221]]

 [[227 221 214]
  [227 221 214]
  [227 221 215]
  {\ldots}
  [232 230 224]
  [231 229 223]
  [230 229 221]]

 {\ldots}

 [[119  81  51]
  [124  85  55]
  [127  87  58]
  {\ldots}
  [210 211 211]
  [211 212 210]
  [210 211 210]]

 [[119  79  51]
  [124  84  55]
  [126  85  56]
  {\ldots}
  [210 211 210]
  [210 211 210]
  [209 210 209]]

 [[119  81  51]
  [123  83  55]
  [122  82  54]
  {\ldots}
  [209 210 210]
  [209 210 209]
  [208 209 209]]], shape=(64, 64, 3), dtype=uint8)
    \end{Verbatim}

    The dataset that you'll be using during this assignment is a subset of
the sign language digits. It contains six different classes representing
the digits from 0 to 5.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{unique\PYZus{}labels} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{element} \PY{o+ow}{in} \PY{n}{y\PYZus{}train}\PY{p}{:}
    \PY{n}{unique\PYZus{}labels}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{element}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{unique\PYZus{}labels}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\{0, 1, 2, 3, 4, 5\}
    \end{Verbatim}

    You can see some of the images in the dataset by running the following
cell.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{images\PYZus{}iter} \PY{o}{=} \PY{n+nb}{iter}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
\PY{n}{labels\PYZus{}iter} \PY{o}{=} \PY{n+nb}{iter}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{)}\PY{p}{:}
    \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n+nb}{next}\PY{p}{(}\PY{n}{images\PYZus{}iter}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{uint8}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n+nb}{next}\PY{p}{(}\PY{n}{labels\PYZus{}iter}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{uint8}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    There's one more additional difference between TensorFlow datasets and
Numpy arrays: If you need to transform one, you would invoke the
\texttt{map} method to apply the function passed as an argument to each
of the elements.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{normalize}\PY{p}{(}\PY{n}{image}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Transform an image into a tensor of shape (64 * 64 * 3, )}
\PY{l+s+sd}{    and normalize its components.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments}
\PY{l+s+sd}{    image \PYZhy{} Tensor.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns: }
\PY{l+s+sd}{    result \PYZhy{}\PYZhy{} Transformed tensor }
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{image} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{cast}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{255.0}
    \PY{n}{image} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{]}\PY{p}{)}
    \PY{k}{return} \PY{n}{image}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{new\PYZus{}train} \PY{o}{=} \PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{normalize}\PY{p}{)}
\PY{n}{new\PYZus{}test} \PY{o}{=} \PY{n}{x\PYZus{}test}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{normalize}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{new\PYZus{}train}\PY{o}{.}\PY{n}{element\PYZus{}spec}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
TensorSpec(shape=(12288,), dtype=tf.float32, name=None)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{next}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{n}{new\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor([0.8901961  0.8627451  0.8392157  {\ldots} 0.8156863  0.81960785
0.81960785], shape=(12288,), dtype=float32)
    \end{Verbatim}

    \#\#\# 2.1 - Linear Function

Let's begin this programming exercise by computing the following
equation: \(Y = WX + b\), where \(W\) and \(X\) are random matrices and
b is a random vector.

\#\#\# Exercise 1 - linear\_function

Compute \(WX + b\) where \(W, X\), and \(b\) are drawn from a random
normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As
an example, this is how to define a constant X with the shape (3,1):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ tf.constant(np.random.randn(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{), name }\OperatorTok{=} \StringTok{"X"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that the difference between \texttt{tf.constant} and
\texttt{tf.Variable} is that you can modify the state of a
\texttt{tf.Variable} but cannot change the state of a
\texttt{tf.constant}.

You might find the following functions helpful: - tf.matmul(\ldots,
\ldots) to do a matrix multiplication - tf.add(\ldots, \ldots) to do an
addition - np.random.randn(\ldots) to initialize randomly

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: linear\PYZus{}function}

\PY{k}{def} \PY{n+nf}{linear\PYZus{}function}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implements a linear function: }
\PY{l+s+sd}{            Initializes X to be a random tensor of shape (3,1)}
\PY{l+s+sd}{            Initializes W to be a random tensor of shape (4,3)}
\PY{l+s+sd}{            Initializes b to be a random tensor of shape (4,1)}
\PY{l+s+sd}{    Returns: }
\PY{l+s+sd}{    result \PYZhy{}\PYZhy{} Y = WX + b }
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Note, to ensure that the \PYZdq{}random\PYZdq{} numbers generated match the expected results,}
\PY{l+s+sd}{    please create the variables in the order given in the starting code below.}
\PY{l+s+sd}{    (Do not re\PYZhy{}arrange the order).}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} (approx. 4 lines)}
    \PY{c+c1}{\PYZsh{} X = ...}
    \PY{c+c1}{\PYZsh{} W = ...}
    \PY{c+c1}{\PYZsh{} b = ...}
    \PY{c+c1}{\PYZsh{} Y = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{X} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{W} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{b} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{Y} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{b}\PY{p}{)}
    

    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{k}{return} \PY{n}{Y}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{result} \PY{o}{=} \PY{n}{linear\PYZus{}function}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{p}{)}

\PY{k}{assert} \PY{n+nb}{type}\PY{p}{(}\PY{n}{result}\PY{p}{)} \PY{o}{==} \PY{n}{EagerTensor}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Use the TensorFlow API}\PY{l+s+s2}{\PYZdq{}}
\PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{p}{[}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.15657382}\PY{p}{]}\PY{p}{,} \PY{p}{[} \PY{l+m+mf}{2.95891446}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.08926781}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.84538042}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor(
[[-2.15657382]
 [ 2.95891446]
 [-1.08926781]
 [-0.84538042]], shape=(4, 1), dtype=float64)
\textcolor{ansi-green-intense}{All test passed}
    \end{Verbatim}

    \textbf{Expected Output}:

\begin{verbatim}
result = 
[[-2.15657382]
 [ 2.95891446]
 [-1.08926781]
 [-0.84538042]]
\end{verbatim}

    \#\#\# 2.2 - Computing the Sigmoid Amazing! You just implemented a
linear function. TensorFlow offers a variety of commonly used neural
network functions like \texttt{tf.sigmoid} and \texttt{tf.softmax}.

For this exercise, compute the sigmoid of z.

In this exercise, you will: Cast your tensor to type \texttt{float32}
using \texttt{tf.cast}, then compute the sigmoid using
\texttt{tf.keras.activations.sigmoid}.

\#\#\# Exercise 2 - sigmoid

Implement the sigmoid function below. You should use the following:

\begin{itemize}
\tightlist
\item
  \texttt{tf.cast("...",\ tf.float32)}
\item
  \texttt{tf.keras.activations.sigmoid("...")}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: sigmoid}

\PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
    
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the sigmoid of z}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    z \PYZhy{}\PYZhy{} input value, scalar or vector}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns: }
\PY{l+s+sd}{    a \PYZhy{}\PYZhy{} (tf.float32) the sigmoid of z}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.}
    
    \PY{c+c1}{\PYZsh{} (approx. 2 lines)}
    \PY{c+c1}{\PYZsh{} z = ...}
    \PY{c+c1}{\PYZsh{} a = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{z} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{cast}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} compute sigmoid(x)}
    \PY{n}{a} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{activations}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}
    
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{k}{return} \PY{n}{a}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{result} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{type: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{result}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dtype: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{dtype}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid(\PYZhy{}1) = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{result}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid(0) = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid(12) = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{sigmoid\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}
    \PY{n}{result} \PY{o}{=} \PY{n}{target}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
    \PY{k}{assert}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{result}\PY{p}{)} \PY{o}{==} \PY{n}{EagerTensor}\PY{p}{)}
    \PY{k}{assert} \PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{dtype} \PY{o}{==} \PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
    \PY{k}{assert} \PY{n}{sigmoid}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{==} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{n}{sigmoid}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{l+m+mf}{0.26894143}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{n}{sigmoid}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{)} \PY{o}{==} \PY{l+m+mf}{0.9999939}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{sigmoid\PYZus{}test}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
type: <class 'tensorflow.python.framework.ops.EagerTensor'>
dtype: <dtype: 'float32'>
sigmoid(-1) = tf.Tensor(0.26894143, shape=(), dtype=float32)
sigmoid(0) = tf.Tensor(0.5, shape=(), dtype=float32)
sigmoid(12) = tf.Tensor(0.9999939, shape=(), dtype=float32)
\textcolor{ansi-green-intense}{All test passed}
    \end{Verbatim}

    \textbf{Expected Output}:

type

class `tensorflow.python.framework.ops.EagerTensor'

dtype

"dtype: `float32'

Sigmoid(-1)

0.2689414

Sigmoid(0)

0.5

Sigmoid(12)

0.999994

    \#\#\# 2.3 - Using One Hot Encodings

Many times in deep learning you will have a \(Y\) vector with numbers
ranging from \(0\) to \(C-1\), where \(C\) is the number of classes. If
\(C\) is for example 4, then you might have the following y vector which
you will need to convert like this:

This is called ``one hot'' encoding, because in the converted
representation, exactly one element of each column is ``hot'' (meaning
set to 1). To do this conversion in numpy, you might have to write a few
lines of code. In TensorFlow, you can use one line of code:

\begin{itemize}
\tightlist
\item
  \href{https://www.tensorflow.org/api_docs/python/tf/one_hot}{tf.one\_hot(labels,
  depth, axis=0)}
\end{itemize}

\texttt{axis=0} indicates the new axis is created at dimension 0

\#\#\# Exercise 3 - one\_hot\_matrix

Implement the function below to take one label and the total number of
classes \(C\), and return the one hot encoding in a column wise matrix.
Use \texttt{tf.one\_hot()} to do this, and \texttt{tf.reshape()} to
reshape your one hot tensor!

\begin{itemize}
\tightlist
\item
  \texttt{tf.reshape(tensor,\ shape)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: one\PYZus{}hot\PYZus{}matrix}
\PY{k}{def} \PY{n+nf}{one\PYZus{}hot\PYZus{}matrix}\PY{p}{(}\PY{n}{label}\PY{p}{,} \PY{n}{depth}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the one hot encoding for a single label}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{        label \PYZhy{}\PYZhy{}  (int) Categorical labels}
\PY{l+s+sd}{        depth \PYZhy{}\PYZhy{}  (int) Number of different classes that label can take}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{         one\PYZus{}hot \PYZhy{}\PYZhy{} tf.Tensor A single\PYZhy{}column matrix with the one hot encoding.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} (approx. 1 line)}
    \PY{c+c1}{\PYZsh{} one\PYZus{}hot = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{one\PYZus{}hot} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(} \PY{n}{tf}\PY{o}{.}\PY{n}{one\PYZus{}hot}\PY{p}{(} \PY{n}{label}\PY{p}{,} \PY{n}{depth}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{depth}\PY{p}{,}\PY{p}{)}\PY{p}{)} 
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{k}{return} \PY{n}{one\PYZus{}hot}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{one\PYZus{}hot\PYZus{}matrix\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}
    \PY{n}{label} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{depth} \PY{o}{=} \PY{l+m+mi}{4}
    \PY{n}{result} \PY{o}{=} \PY{n}{target}\PY{p}{(}\PY{n}{label}\PY{p}{,} \PY{n}{depth}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test 1:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{result}\PY{p}{)}
    \PY{k}{assert} \PY{n}{result}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{==} \PY{n}{depth}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Use the parameter depth}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{1.} \PY{p}{,}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{0.}\PY{p}{]} \PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Wrong output. Use tf.one\PYZus{}hot}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{label\PYZus{}2} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
    \PY{n}{result} \PY{o}{=} \PY{n}{target}\PY{p}{(}\PY{n}{label\PYZus{}2}\PY{p}{,} \PY{n}{depth}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test 2:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{result}\PY{p}{)}
    \PY{k}{assert} \PY{n}{result}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{==} \PY{n}{depth}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Use the parameter depth}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{0.} \PY{p}{,}\PY{l+m+mf}{1.}\PY{p}{,} \PY{l+m+mf}{0.}\PY{p}{]} \PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Wrong output. Use tf.reshape as instructed}\PY{l+s+s2}{\PYZdq{}}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{one\PYZus{}hot\PYZus{}matrix\PYZus{}test}\PY{p}{(}\PY{n}{one\PYZus{}hot\PYZus{}matrix}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Test 1: tf.Tensor([0. 1. 0. 0.], shape=(4,), dtype=float32)
Test 2: tf.Tensor([0. 0. 1. 0.], shape=(4,), dtype=float32)
\textcolor{ansi-green-intense}{All test passed}
    \end{Verbatim}

    \textbf{Expected output}

\begin{verbatim}
Test 1: tf.Tensor([0. 1. 0. 0.], shape=(4,), dtype=float32)
Test 2: tf.Tensor([0. 0. 1. 0.], shape=(4,), dtype=float32)
\end{verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{new\PYZus{}y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{one\PYZus{}hot\PYZus{}matrix}\PY{p}{)}
\PY{n}{new\PYZus{}y\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{one\PYZus{}hot\PYZus{}matrix}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{next}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{n}{new\PYZus{}y\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor([1. 0. 0. 0. 0. 0.], shape=(6,), dtype=float32)
    \end{Verbatim}

    \#\#\# 2.4 - Initialize the Parameters

Now you'll initialize a vector of numbers with the Glorot initializer.
The function you'll be calling is
\texttt{tf.keras.initializers.GlorotNormal}, which draws samples from a
truncated normal distribution centered on 0, with
\texttt{stddev\ =\ sqrt(2\ /\ (fan\_in\ +\ fan\_out))}, where
\texttt{fan\_in} is the number of input units and \texttt{fan\_out} is
the number of output units, both in the weight tensor.

To initialize with zeros or ones you could use \texttt{tf.zeros()} or
\texttt{tf.ones()} instead.

\#\#\# Exercise 4 - initialize\_parameters

Implement the function below to take in a shape and to return an array
of numbers using the GlorotNormal initializer.

\begin{itemize}
\tightlist
\item
  \texttt{tf.keras.initializers.GlorotNormal(seed=1)}
\item
  \texttt{tf.Variable(initializer(shape=())}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: initialize\PYZus{}parameters}

\PY{k}{def} \PY{n+nf}{initialize\PYZus{}parameters}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Initializes parameters to build a neural network with TensorFlow. The shapes are:}
\PY{l+s+sd}{                        W1 : [25, 12288]}
\PY{l+s+sd}{                        b1 : [25, 1]}
\PY{l+s+sd}{                        W2 : [12, 25]}
\PY{l+s+sd}{                        b2 : [12, 1]}
\PY{l+s+sd}{                        W3 : [6, 12]}
\PY{l+s+sd}{                        b3 : [6, 1]}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} a dictionary of tensors containing W1, b1, W2, b2, W3, b3}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
                                
    \PY{n}{initializer} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{initializers}\PY{o}{.}\PY{n}{GlorotNormal}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}   
    \PY{c+c1}{\PYZsh{}(approx. 6 lines of code)}
    \PY{n}{W1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{initializer}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{l+m+mi}{12288}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{b1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{initializer}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{W2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{initializer}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{25}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{b2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{initializer}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{W3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{initializer}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{b3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{initializer}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}W1 = tf.Variable(\PYZdq{}W1\PYZdq{},[25,12288],initializer)}
 
    \PY{c+c1}{\PYZsh{} W1 = ...}
    \PY{c+c1}{\PYZsh{} b1 = ...}
    \PY{c+c1}{\PYZsh{} W2 = ...}
    \PY{c+c1}{\PYZsh{} b2 = ...}
    \PY{c+c1}{\PYZsh{} W3 = ...}
    \PY{c+c1}{\PYZsh{} b3 = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}

    \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{W1}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{b1}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{W2}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{b2}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{W3}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{b3}\PY{p}{\PYZcb{}}
    
    \PY{k}{return} \PY{n}{parameters}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{initialize\PYZus{}parameters\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}
    \PY{n}{parameters} \PY{o}{=} \PY{n}{target}\PY{p}{(}\PY{p}{)}

    \PY{n}{values} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{12288}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{\PYZcb{}}

    \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{parameters}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{key}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ shape: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{tuple}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{k}{assert} \PY{n+nb}{type}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)} \PY{o}{==} \PY{n}{ResourceVariable}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{All parameter must be created using tf.Variable}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{assert} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{o}{==} \PY{n}{values}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{key}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: wrong shape}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{l+m+mf}{0.5}\PY{p}{,}  \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{key}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: Use the GlorotNormal initializer}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{o+ow}{and} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{parameters}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{key}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: Use the GlorotNormal initializer}\PY{l+s+s2}{\PYZdq{}}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
\PY{n}{initialize\PYZus{}parameters\PYZus{}test}\PY{p}{(}\PY{n}{initialize\PYZus{}parameters}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
W1 shape: (25, 12288)
b1 shape: (25, 1)
W2 shape: (12, 25)
b2 shape: (12, 1)
W3 shape: (6, 12)
b3 shape: (6, 1)
\textcolor{ansi-green-intense}{All test passed}
    \end{Verbatim}

    \textbf{Expected output}

\begin{verbatim}
W1 shape: (25, 12288)
b1 shape: (25, 1)
W2 shape: (12, 25)
b2 shape: (12, 1)
W3 shape: (6, 12)
b3 shape: (6, 1)
\end{verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{parameters} \PY{o}{=} \PY{n}{initialize\PYZus{}parameters}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \#\# 3 - Building Your First Neural Network in TensorFlow

In this part of the assignment you will build a neural network using
TensorFlow. Remember that there are two parts to implementing a
TensorFlow model:

\begin{itemize}
\tightlist
\item
  Implement forward propagation
\item
  Retrieve the gradients and train the model
\end{itemize}

Let's get into it!

    \#\#\# 3.1 - Implement Forward Propagation

One of TensorFlow's great strengths lies in the fact that you only need
to implement the forward propagation function and it will keep track of
the operations you did to calculate the back propagation automatically.

\#\#\# Exercise 5 - forward\_propagation

Implement the \texttt{forward\_propagation} function.

\textbf{Note} Use only the TF API.

\begin{itemize}
\tightlist
\item
  tf.math.add
\item
  tf.linalg.matmul
\item
  tf.keras.activations.relu
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: forward\PYZus{}propagation}

\PY{k}{def} \PY{n+nf}{forward\PYZus{}propagation}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implements the forward propagation for the model: LINEAR \PYZhy{}\PYZgt{} RELU \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} RELU \PYZhy{}\PYZgt{} LINEAR}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    X \PYZhy{}\PYZhy{} input dataset placeholder, of shape (input size, number of examples)}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} python dictionary containing your parameters \PYZdq{}W1\PYZdq{}, \PYZdq{}b1\PYZdq{}, \PYZdq{}W2\PYZdq{}, \PYZdq{}b2\PYZdq{}, \PYZdq{}W3\PYZdq{}, \PYZdq{}b3\PYZdq{}}
\PY{l+s+sd}{                  the shapes are given in initialize\PYZus{}parameters}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    Z3 \PYZhy{}\PYZhy{} the output of the last LINEAR unit}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} Retrieve the parameters from the dictionary \PYZdq{}parameters\PYZdq{} }
    \PY{n}{W1} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b1} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{W2} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b2} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{W3} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b3} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{}(approx. 5 lines)                   \PYZsh{} Numpy Equivalents:}
    \PY{c+c1}{\PYZsh{} Z1 = ...                           \PYZsh{} Z1 = np.dot(W1, X) + b1}
    \PY{c+c1}{\PYZsh{} A1 = ...                           \PYZsh{} A1 = relu(Z1)}
    \PY{c+c1}{\PYZsh{} Z2 = ...                           \PYZsh{} Z2 = np.dot(W2, A1) + b2}
    \PY{c+c1}{\PYZsh{} A2 = ...                           \PYZsh{} A2 = relu(Z2)}
    \PY{c+c1}{\PYZsh{} Z3 = ...                           \PYZsh{} Z3 = np.dot(W3, A2) + b3}
    \PY{n}{Z1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{W1}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{n}{b1}\PY{p}{)}                                             \PY{c+c1}{\PYZsh{} Z1 = np.dot(W1, X) + b1}
    \PY{n}{A1} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n}{Z1}\PY{p}{)}                                              \PY{c+c1}{\PYZsh{} A1 = relu(Z1)}
    \PY{n}{Z2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{W2}\PY{p}{,}\PY{n}{A1}\PY{p}{)}\PY{p}{,}\PY{n}{b2}\PY{p}{)}                                                \PY{c+c1}{\PYZsh{} Z2 = np.dot(W2, a1) + b2}
    \PY{n}{A2} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n}{Z2}\PY{p}{)}                                              \PY{c+c1}{\PYZsh{} A2 = relu(Z2)}
    \PY{n}{Z3} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{W3}\PY{p}{,}\PY{n}{A2}\PY{p}{)}\PY{p}{,}\PY{n}{b3}\PY{p}{)} 
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{k}{return} \PY{n}{Z3}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{forward\PYZus{}propagation\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{,} \PY{n}{examples}\PY{p}{)}\PY{p}{:}
    \PY{n}{minibatches} \PY{o}{=} \PY{n}{examples}\PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{parametersk} \PY{o}{=} \PY{n}{initialize\PYZus{}parameters}\PY{p}{(}\PY{p}{)}
    \PY{n}{W1} \PY{o}{=} \PY{n}{parametersk}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b1} \PY{o}{=} \PY{n}{parametersk}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{W2} \PY{o}{=} \PY{n}{parametersk}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b2} \PY{o}{=} \PY{n}{parametersk}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{W3} \PY{o}{=} \PY{n}{parametersk}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b3} \PY{o}{=} \PY{n}{parametersk}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{index} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{minibatch} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{minibatches}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{GradientTape}\PY{p}{(}\PY{p}{)} \PY{k}{as} \PY{n}{tape}\PY{p}{:}
        \PY{n}{forward\PYZus{}pass} \PY{o}{=} \PY{n}{target}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{minibatch}\PY{p}{)}\PY{p}{,} \PY{n}{parametersk}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{forward\PYZus{}pass}\PY{p}{)}
        \PY{n}{fake\PYZus{}cost} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reduce\PYZus{}mean}\PY{p}{(}\PY{n}{forward\PYZus{}pass} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}

        \PY{k}{assert} \PY{n+nb}{type}\PY{p}{(}\PY{n}{forward\PYZus{}pass}\PY{p}{)} \PY{o}{==} \PY{n}{EagerTensor}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Your output is not a tensor}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{assert} \PY{n}{forward\PYZus{}pass}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Last layer must use W3 and b3}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{forward\PYZus{}pass}\PY{p}{,} 
                           \PY{p}{[}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.13430887}\PY{p}{,}  \PY{l+m+mf}{0.14086473}\PY{p}{]}\PY{p}{,}
                            \PY{p}{[} \PY{l+m+mf}{0.21588647}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.02582335}\PY{p}{]}\PY{p}{,}
                            \PY{p}{[} \PY{l+m+mf}{0.7059658}\PY{p}{,}   \PY{l+m+mf}{0.6484556} \PY{p}{]}\PY{p}{,}
                            \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.1260961}\PY{p}{,}  \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.9329492} \PY{p}{]}\PY{p}{,}
                            \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.20181894}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.3382722} \PY{p}{]}\PY{p}{,}
                            \PY{p}{[} \PY{l+m+mf}{0.9558965}\PY{p}{,}   \PY{l+m+mf}{0.94167566}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output does not match}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{index} \PY{o}{=} \PY{n}{index} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{trainable\PYZus{}variables} \PY{o}{=} \PY{p}{[}\PY{n}{W1}\PY{p}{,} \PY{n}{b1}\PY{p}{,} \PY{n}{W2}\PY{p}{,} \PY{n}{b2}\PY{p}{,} \PY{n}{W3}\PY{p}{,} \PY{n}{b3}\PY{p}{]}
    \PY{n}{grads} \PY{o}{=} \PY{n}{tape}\PY{o}{.}\PY{n}{gradient}\PY{p}{(}\PY{n}{fake\PYZus{}cost}\PY{p}{,} \PY{n}{trainable\PYZus{}variables}\PY{p}{)}
    \PY{k}{assert} \PY{o+ow}{not}\PY{p}{(}\PY{k+kc}{None} \PY{o+ow}{in} \PY{n}{grads}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Wrong gradients. It could be due to the use of tf.Variable whithin forward\PYZus{}propagation}\PY{l+s+s2}{\PYZdq{}}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{forward\PYZus{}propagation\PYZus{}test}\PY{p}{(}\PY{n}{forward\PYZus{}propagation}\PY{p}{,} \PY{n}{new\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor(
[[-0.13430887  0.14086473]
 [ 0.21588647 -0.02582335]
 [ 0.7059658   0.6484556 ]
 [-1.1260961  -0.9329492 ]
 [-0.20181894 -0.3382722 ]
 [ 0.9558965   0.94167566]], shape=(6, 2), dtype=float32)
\textcolor{ansi-green-intense}{All test passed}
    \end{Verbatim}

    \textbf{Expected output}

\begin{verbatim}
tf.Tensor(
[[-0.13430887  0.14086473]
 [ 0.21588647 -0.02582335]
 [ 0.7059658   0.6484556 ]
 [-1.1260961  -0.9329492 ]
 [-0.20181894 -0.3382722 ]
 [ 0.9558965   0.94167566]], shape=(6, 2), dtype=float32)
\end{verbatim}

    \#\#\# 3.2 Compute the Cost

All you have to do now is define the loss function that you're going to
use. For this case, since we have a classification problem with 6
labels, a categorical cross entropy will work!

\#\#\# Exercise 6 - compute\_cost

Implement the cost function below. - It's important to note that the
``\texttt{y\_pred}'' and ``\texttt{y\_true}'' inputs of
\href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy}{tf.keras.losses.categorical\_crossentropy}
are expected to be of shape (number of examples, num\_classes).

\begin{itemize}
\tightlist
\item
  \texttt{tf.reduce\_sum} does the summation over the examples.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}cost }

\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{logits}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the cost}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    logits \PYZhy{}\PYZhy{} output of forward propagation (output of the last LINEAR unit), of shape (6, num\PYZus{}examples)}
\PY{l+s+sd}{    labels \PYZhy{}\PYZhy{} \PYZdq{}true\PYZdq{} labels vector, same shape as Z3}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    cost \PYZhy{} Tensor of the cost function}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}(1 line of code)}
    \PY{c+c1}{\PYZsh{} cost = ...}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{logits} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{logits}\PY{p}{)}
    \PY{n}{labels} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{labels}\PY{p}{)}\PY{p}{,}\PY{p}{[}\PY{n}{logits}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}
    \PY{n}{cost} \PY{o}{=}  \PY{n}{tf}\PY{o}{.}\PY{n}{reduce\PYZus{}mean}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{losses}\PY{o}{.}\PY{n}{categorical\PYZus{}crossentropy}\PY{p}{(}\PY{n}{labels}\PY{p}{,} \PY{n}{logits}\PY{p}{,}\PY{n}{from\PYZus{}logits}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{k}{return} \PY{n}{cost}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{,} \PY{n}{Y}\PY{p}{)}\PY{p}{:}
    \PY{n}{pred} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{p}{[}\PY{p}{[} \PY{l+m+mf}{2.4048107}\PY{p}{,}   \PY{l+m+mf}{5.0334096} \PY{p}{]}\PY{p}{,}
             \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.7921977}\PY{p}{,}  \PY{o}{\PYZhy{}}\PY{l+m+mf}{4.1523376} \PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{0.9447198}\PY{p}{,}  \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.46802214}\PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{1.158121}\PY{p}{,}    \PY{l+m+mf}{3.9810789} \PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{4.768706}\PY{p}{,}    \PY{l+m+mf}{2.3220146} \PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{6.1481323}\PY{p}{,}   \PY{l+m+mf}{3.909829}  \PY{p}{]}\PY{p}{]}\PY{p}{)}
    \PY{n}{minibatches} \PY{o}{=} \PY{n}{Y}\PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
    \PY{k}{for} \PY{n}{minibatch} \PY{o+ow}{in} \PY{n}{minibatches}\PY{p}{:}
        \PY{n}{result} \PY{o}{=} \PY{n}{target}\PY{p}{(}\PY{n}{pred}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{minibatch}\PY{p}{)}\PY{p}{)}
        \PY{k}{break}
        
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{p}{)}
    \PY{k}{assert}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{result}\PY{p}{)} \PY{o}{==} \PY{n}{EagerTensor}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Use the TensorFlow API}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{result} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mf}{0.25361037} \PY{o}{+} \PY{l+m+mf}{0.5566767}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{2.0}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test does not match. Did you get the mean of your cost functions?}\PY{l+s+s2}{\PYZdq{}}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{compute\PYZus{}cost\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}cost}\PY{p}{,} \PY{n}{new\PYZus{}y\PYZus{}train} \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor(0.4051435, shape=(), dtype=float32)
\textcolor{ansi-green-intense}{All test passed}
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{,} \PY{n}{Y}\PY{p}{)}\PY{p}{:}
    \PY{n}{pred} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{constant}\PY{p}{(}\PY{p}{[}\PY{p}{[} \PY{l+m+mf}{2.4048107}\PY{p}{,}   \PY{l+m+mf}{5.0334096} \PY{p}{]}\PY{p}{,}
             \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.7921977}\PY{p}{,}  \PY{o}{\PYZhy{}}\PY{l+m+mf}{4.1523376} \PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{0.9447198}\PY{p}{,}  \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.46802214}\PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{1.158121}\PY{p}{,}    \PY{l+m+mf}{3.9810789} \PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{4.768706}\PY{p}{,}    \PY{l+m+mf}{2.3220146} \PY{p}{]}\PY{p}{,}
             \PY{p}{[} \PY{l+m+mf}{6.1481323}\PY{p}{,}   \PY{l+m+mf}{3.909829}  \PY{p}{]}\PY{p}{]}\PY{p}{)}
    \PY{n}{minibatches} \PY{o}{=} \PY{n}{Y}\PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
    \PY{k}{for} \PY{n}{minibatch} \PY{o+ow}{in} \PY{n}{minibatches}\PY{p}{:}
        \PY{n}{result} \PY{o}{=} \PY{n}{target}\PY{p}{(}\PY{n}{pred}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{minibatch}\PY{p}{)}\PY{p}{)}
        \PY{k}{break}
        
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{p}{)}
    \PY{k}{assert}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{result}\PY{p}{)} \PY{o}{==} \PY{n}{EagerTensor}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Use the TensorFlow API}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{result} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mf}{0.50722074} \PY{o}{+} \PY{l+m+mf}{1.1133534}\PY{p}{)} \PY{o}{/} \PY{l+m+mf}{2.0}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test does not match. Did you get the reduce sum of your cost functions?}\PY{l+s+s2}{\PYZdq{}}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{compute\PYZus{}cost\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}cost}\PY{p}{,} \PY{n}{new\PYZus{}y\PYZus{}train} \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tf.Tensor(0.4051435, shape=(), dtype=float32)
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        AssertionError                            Traceback (most recent call last)

        <ipython-input-35-501532b5db98> in <module>
         17     print("\textbackslash{}033[92mAll test passed")
         18 
    ---> 19 compute\_cost\_test(compute\_cost, new\_y\_train )
    

        <ipython-input-35-501532b5db98> in compute\_cost\_test(target, Y)
         13     print(result)
         14     assert(type(result) == EagerTensor), "Use the TensorFlow API"
    ---> 15     assert (np.abs(result - (0.50722074 + 1.1133534) / 2.0) < 1e-7), "Test does not match. Did you get the reduce sum of your cost functions?"
         16 
         17     print("\textbackslash{}033[92mAll test passed")


        AssertionError: Test does not match. Did you get the reduce sum of your cost functions?

    \end{Verbatim}

    \textbf{Expected output}

\begin{verbatim}
tf.Tensor(0.810287, shape=(), dtype=float32)
\end{verbatim}

    \#\#\# 3.3 - Train the Model

Let's talk optimizers. You'll specify the type of optimizer in one line,
in this case \texttt{tf.keras.optimizers.Adam} (though you can use
others such as SGD), and then call it within the training loop.

Notice the \texttt{tape.gradient} function: this allows you to retrieve
the operations recorded for automatic differentiation inside the
\texttt{GradientTape} block. Then, calling the optimizer method
\texttt{apply\_gradients}, will apply the optimizer's update rules to
each trainable parameter. At the end of this assignment, you'll find
some documentation that explains this more in detail, but for now, a
simple explanation will do. ;)

Here you should take note of an important extra step that's been added
to the batch training process:

\begin{itemize}
\tightlist
\item
  \texttt{tf.Data.dataset\ =\ dataset.prefetch(8)}
\end{itemize}

What this does is prevent a memory bottleneck that can occur when
reading from disk. \texttt{prefetch()} sets aside some data and keeps it
ready for when it's needed. It does this by creating a source dataset
from your input data, applying a transformation to preprocess the data,
then iterating over the dataset the specified number of elements at a
time. This works because the iteration is streaming, so the data doesn't
need to fit into the memory.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{model}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.0001}\PY{p}{,}
          \PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{1500}\PY{p}{,} \PY{n}{minibatch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{print\PYZus{}cost} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implements a three\PYZhy{}layer tensorflow neural network: LINEAR\PYZhy{}\PYZgt{}RELU\PYZhy{}\PYZgt{}LINEAR\PYZhy{}\PYZgt{}RELU\PYZhy{}\PYZgt{}LINEAR\PYZhy{}\PYZgt{}SOFTMAX.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    X\PYZus{}train \PYZhy{}\PYZhy{} training set, of shape (input size = 12288, number of training examples = 1080)}
\PY{l+s+sd}{    Y\PYZus{}train \PYZhy{}\PYZhy{} test set, of shape (output size = 6, number of training examples = 1080)}
\PY{l+s+sd}{    X\PYZus{}test \PYZhy{}\PYZhy{} training set, of shape (input size = 12288, number of training examples = 120)}
\PY{l+s+sd}{    Y\PYZus{}test \PYZhy{}\PYZhy{} test set, of shape (output size = 6, number of test examples = 120)}
\PY{l+s+sd}{    learning\PYZus{}rate \PYZhy{}\PYZhy{} learning rate of the optimization}
\PY{l+s+sd}{    num\PYZus{}epochs \PYZhy{}\PYZhy{} number of epochs of the optimization loop}
\PY{l+s+sd}{    minibatch\PYZus{}size \PYZhy{}\PYZhy{} size of a minibatch}
\PY{l+s+sd}{    print\PYZus{}cost \PYZhy{}\PYZhy{} True to print the cost every 10 epochs}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} parameters learnt by the model. They can then be used to predict.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{costs} \PY{o}{=} \PY{p}{[}\PY{p}{]}                                        \PY{c+c1}{\PYZsh{} To keep track of the cost}
    \PY{n}{train\PYZus{}acc} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{test\PYZus{}acc} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Initialize your parameters}
    \PY{c+c1}{\PYZsh{}(1 line)}
    \PY{n}{parameters} \PY{o}{=} \PY{n}{initialize\PYZus{}parameters}\PY{p}{(}\PY{p}{)}

    \PY{n}{W1} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b1} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{W2} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b2} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{W3} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
    \PY{n}{b3} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

    \PY{n}{optimizer} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} The CategoricalAccuracy will track the accuracy for this multiclass problem}
    \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{metrics}\PY{o}{.}\PY{n}{CategoricalAccuracy}\PY{p}{(}\PY{p}{)}
    \PY{n}{train\PYZus{}accuracy} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{metrics}\PY{o}{.}\PY{n}{CategoricalAccuracy}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{dataset} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{o}{.}\PY{n}{zip}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}\PY{p}{)}
    \PY{n}{test\PYZus{}dataset} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{o}{.}\PY{n}{zip}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}test}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} We can get the number of elements of a dataset using the cardinality method}
    \PY{n}{m} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{cardinality}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{minibatches} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{n}{minibatch\PYZus{}size}\PY{p}{)}\PY{o}{.}\PY{n}{prefetch}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
    \PY{n}{test\PYZus{}minibatches} \PY{o}{=} \PY{n}{test\PYZus{}dataset}\PY{o}{.}\PY{n}{batch}\PY{p}{(}\PY{n}{minibatch\PYZus{}size}\PY{p}{)}\PY{o}{.}\PY{n}{prefetch}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}X\PYZus{}train = X\PYZus{}train.batch(minibatch\PYZus{}size, drop\PYZus{}remainder=True).prefetch(8)\PYZsh{} \PYZlt{}\PYZlt{}\PYZlt{} extra step    }
    \PY{c+c1}{\PYZsh{}Y\PYZus{}train = Y\PYZus{}train.batch(minibatch\PYZus{}size, drop\PYZus{}remainder=True).prefetch(8) \PYZsh{} loads memory faster }

    \PY{c+c1}{\PYZsh{} Do the training loop}
    \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}

        \PY{n}{epoch\PYZus{}cost} \PY{o}{=} \PY{l+m+mf}{0.}
        
        \PY{c+c1}{\PYZsh{}We need to reset object to start measuring from 0 the accuracy each epoch}
        \PY{n}{train\PYZus{}accuracy}\PY{o}{.}\PY{n}{reset\PYZus{}states}\PY{p}{(}\PY{p}{)}
        
        \PY{k}{for} \PY{p}{(}\PY{n}{minibatch\PYZus{}X}\PY{p}{,} \PY{n}{minibatch\PYZus{}Y}\PY{p}{)} \PY{o+ow}{in} \PY{n}{minibatches}\PY{p}{:}
            
            \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{GradientTape}\PY{p}{(}\PY{p}{)} \PY{k}{as} \PY{n}{tape}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} 1. predict}
                \PY{n}{Z3} \PY{o}{=} \PY{n}{forward\PYZus{}propagation}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{minibatch\PYZus{}X}\PY{p}{)}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}

                \PY{c+c1}{\PYZsh{} 2. loss}
                \PY{n}{minibatch\PYZus{}cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{Z3}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{minibatch\PYZus{}Y}\PY{p}{)}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} We accumulate the accuracy of all the batches}
            \PY{n}{train\PYZus{}accuracy}\PY{o}{.}\PY{n}{update\PYZus{}state}\PY{p}{(}\PY{n}{minibatch\PYZus{}Y}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{Z3}\PY{p}{)}\PY{p}{)}
            
            \PY{n}{trainable\PYZus{}variables} \PY{o}{=} \PY{p}{[}\PY{n}{W1}\PY{p}{,} \PY{n}{b1}\PY{p}{,} \PY{n}{W2}\PY{p}{,} \PY{n}{b2}\PY{p}{,} \PY{n}{W3}\PY{p}{,} \PY{n}{b3}\PY{p}{]}
            \PY{n}{grads} \PY{o}{=} \PY{n}{tape}\PY{o}{.}\PY{n}{gradient}\PY{p}{(}\PY{n}{minibatch\PYZus{}cost}\PY{p}{,} \PY{n}{trainable\PYZus{}variables}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{apply\PYZus{}gradients}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{grads}\PY{p}{,} \PY{n}{trainable\PYZus{}variables}\PY{p}{)}\PY{p}{)}
            \PY{n}{epoch\PYZus{}cost} \PY{o}{+}\PY{o}{=} \PY{n}{minibatch\PYZus{}cost}
        
        \PY{c+c1}{\PYZsh{} We divide the epoch cost over the number of samples}
        \PY{n}{epoch\PYZus{}cost} \PY{o}{/}\PY{o}{=} \PY{n}{m}

        \PY{c+c1}{\PYZsh{} Print the cost every 10 epochs}
        \PY{k}{if} \PY{n}{print\PYZus{}cost} \PY{o}{==} \PY{k+kc}{True} \PY{o+ow}{and} \PY{n}{epoch} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cost after epoch }\PY{l+s+si}{\PYZpc{}i}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{epoch}\PY{p}{,} \PY{n}{epoch\PYZus{}cost}\PY{p}{)}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{train\PYZus{}accuracy}\PY{o}{.}\PY{n}{result}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} We evaluate the test set every 10 epochs to avoid computational overhead}
            \PY{k}{for} \PY{p}{(}\PY{n}{minibatch\PYZus{}X}\PY{p}{,} \PY{n}{minibatch\PYZus{}Y}\PY{p}{)} \PY{o+ow}{in} \PY{n}{test\PYZus{}minibatches}\PY{p}{:}
                \PY{n}{Z3} \PY{o}{=} \PY{n}{forward\PYZus{}propagation}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{minibatch\PYZus{}X}\PY{p}{)}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
                \PY{n}{test\PYZus{}accuracy}\PY{o}{.}\PY{n}{update\PYZus{}state}\PY{p}{(}\PY{n}{minibatch\PYZus{}Y}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{Z3}\PY{p}{)}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test\PYZus{}accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{test\PYZus{}accuracy}\PY{o}{.}\PY{n}{result}\PY{p}{(}\PY{p}{)}\PY{p}{)}

            \PY{n}{costs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{epoch\PYZus{}cost}\PY{p}{)}
            \PY{n}{train\PYZus{}acc}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{train\PYZus{}accuracy}\PY{o}{.}\PY{n}{result}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{test\PYZus{}acc}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{test\PYZus{}accuracy}\PY{o}{.}\PY{n}{result}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{test\PYZus{}accuracy}\PY{o}{.}\PY{n}{reset\PYZus{}states}\PY{p}{(}\PY{p}{)}


    \PY{k}{return} \PY{n}{parameters}\PY{p}{,} \PY{n}{costs}\PY{p}{,} \PY{n}{train\PYZus{}acc}\PY{p}{,} \PY{n}{test\PYZus{}acc}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{parameters}\PY{p}{,} \PY{n}{costs}\PY{p}{,} \PY{n}{train\PYZus{}acc}\PY{p}{,} \PY{n}{test\PYZus{}acc} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{new\PYZus{}train}\PY{p}{,} \PY{n}{new\PYZus{}y\PYZus{}train}\PY{p}{,} \PY{n}{new\PYZus{}test}\PY{p}{,} \PY{n}{new\PYZus{}y\PYZus{}test}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cost after epoch 0: 0.057612
Train accuracy: tf.Tensor(0.17314816, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.24166666, shape=(), dtype=float32)
Cost after epoch 10: 0.049332
Train accuracy: tf.Tensor(0.35833332, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.3, shape=(), dtype=float32)
Cost after epoch 20: 0.043173
Train accuracy: tf.Tensor(0.49907407, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.43333334, shape=(), dtype=float32)
Cost after epoch 30: 0.037322
Train accuracy: tf.Tensor(0.60462964, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.525, shape=(), dtype=float32)
Cost after epoch 40: 0.033147
Train accuracy: tf.Tensor(0.6490741, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.5416667, shape=(), dtype=float32)
Cost after epoch 50: 0.030203
Train accuracy: tf.Tensor(0.68333334, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.625, shape=(), dtype=float32)
Cost after epoch 60: 0.028050
Train accuracy: tf.Tensor(0.6935185, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.625, shape=(), dtype=float32)
Cost after epoch 70: 0.026298
Train accuracy: tf.Tensor(0.72407407, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.64166665, shape=(), dtype=float32)
Cost after epoch 80: 0.024799
Train accuracy: tf.Tensor(0.7425926, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.68333334, shape=(), dtype=float32)
Cost after epoch 90: 0.023551
Train accuracy: tf.Tensor(0.75277776, shape=(), dtype=float32)
Test\_accuracy: tf.Tensor(0.68333334, shape=(), dtype=float32)
    \end{Verbatim}

    \textbf{Expected output}

\begin{verbatim}
Cost after epoch 0: 1.830244
Train accuracy: tf.Tensor(0.17037037, shape=(), dtype=float32)
Test_accuracy: tf.Tensor(0.2, shape=(), dtype=float32)
Cost after epoch 10: 1.552390
Train accuracy: tf.Tensor(0.35925925, shape=(), dtype=float32)
Test_accuracy: tf.Tensor(0.30833334, shape=(), dtype=float32)
...
\end{verbatim}

Numbers you get can be different, just check that your loss is going
down and your accuracy going up!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot the cost}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{costs}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cost}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iterations (per fives)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Learning rate =}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot the train accuracy}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{train\PYZus{}acc}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iterations (per fives)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Learning rate =}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Plot the test accuracy}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{test\PYZus{}acc}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iterations (per fives)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Learning rate =}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_57_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Congratulations}! You've made it to the end of this assignment,
and to the end of this week's material. Amazing work building a neural
network in TensorFlow 2.3!

Here's a quick recap of all you just achieved:

\begin{itemize}
\tightlist
\item
  Used \texttt{tf.Variable} to modify your variables
\item
  Trained a Neural Network on a TensorFlow dataset
\end{itemize}

You are now able to harness the power of TensorFlow to create cool
things, faster. Nice!

    \#\# 4 - Bibliography

In this assignment, you were introducted to \texttt{tf.GradientTape},
which records operations for differentation. Here are a couple of
resources for diving deeper into what it does and why:

Introduction to Gradients and Automatic Differentiation:
https://www.tensorflow.org/guide/autodiff

GradientTape documentation:
https://www.tensorflow.org/api\_docs/python/tf/GradientTape

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
