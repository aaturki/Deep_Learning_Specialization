\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Regularization}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{regularization}{%
\section{Regularization}\label{regularization}}

Welcome to the second assignment of this week. Deep Learning models have
so much flexibility and capacity that \textbf{overfitting can be a
serious problem}, if the training dataset is not big enough. Sure it
does well on the training set, but the learned network \textbf{doesn't
generalize to new examples} that it has never seen!

\textbf{You will learn to:} Use regularization in your deep learning
models.

Let's get started!

\hypertarget{important-note-on-submission-to-the-autograder}{%
\subsection{Important Note on Submission to the
AutoGrader}\label{important-note-on-submission-to-the-autograder}}

Before submitting your assignment to the AutoGrader, please make sure
you are not doing the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You have not added any \emph{extra} \texttt{print} statement(s) in the
  assignment.
\item
  You have not added any \emph{extra} code cell(s) in the assignment.
\item
  You have not changed any of the function parameters.
\item
  You are not using any global variables inside your graded exercises.
  Unless specifically instructed to do so, please refrain from it and
  use the local variables instead.
\item
  You are not changing the assignment code where it is not required,
  like creating \emph{extra} variables.
\end{enumerate}

If you do any of the following, you will get something like,
\texttt{Grader\ not\ found} (or similarly unexpected) error upon
submitting your assignment. Before asking for help/debugging the errors
in your assignment, check for these first. If this is the case, and you
don't remember the changes you have made, you can get a fresh copy of
the assignment by following these
\href{https://www.coursera.org/learn/deep-neural-network/supplement/QWEnZ/h-ow-to-refresh-your-workspace}{instructions}.

    \hypertarget{table-of-contents}{%
\subsection{Table of Contents}\label{table-of-contents}}

\begin{itemize}
\tightlist
\item
  Section \ref{1}
\item
  Section \ref{2}
\item
  Section \ref{3}
\item
  Section \ref{4}
\item
  Section \ref{5}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex-1}
  \item
    Section \ref{ex-2}
  \end{itemize}
\item
  Section \ref{6}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{6-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-3}
    \end{itemize}
  \item
    Section \ref{6-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-4}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{7}
\end{itemize}

    \#\# 1 - Packages

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import packages}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{sklearn}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{io}
\PY{k+kn}{from} \PY{n+nn}{reg\PYZus{}utils} \PY{k+kn}{import} \PY{n}{sigmoid}\PY{p}{,} \PY{n}{relu}\PY{p}{,} \PY{n}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{,} \PY{n}{initialize\PYZus{}parameters}\PY{p}{,} \PY{n}{load\PYZus{}2D\PYZus{}dataset}\PY{p}{,} \PY{n}{predict\PYZus{}dec}
\PY{k+kn}{from} \PY{n+nn}{reg\PYZus{}utils} \PY{k+kn}{import} \PY{n}{compute\PYZus{}cost}\PY{p}{,} \PY{n}{predict}\PY{p}{,} \PY{n}{forward\PYZus{}propagation}\PY{p}{,} \PY{n}{backward\PYZus{}propagation}\PY{p}{,} \PY{n}{update\PYZus{}parameters}
\PY{k+kn}{from} \PY{n+nn}{testCases} \PY{k+kn}{import} \PY{o}{*}
\PY{k+kn}{from} \PY{n+nn}{public\PYZus{}tests} \PY{k+kn}{import} \PY{o}{*}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{7.0}\PY{p}{,} \PY{l+m+mf}{4.0}\PY{p}{)} \PY{c+c1}{\PYZsh{} set default size of plots}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image.interpolation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image.cmap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}

\PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
\PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
\end{Verbatim}
\end{tcolorbox}

    \#\# 2 - Problem Statement

    You have just been hired as an AI expert by the French Football
Corporation. They would like you to recommend positions where France's
goal keeper should kick the ball so that the French team's players can
then hit it with their head.

Figure 1: Football field. The goal keeper kicks the ball in the air, the
players of each team are fighting to hit the ball with their head

They give you the following 2D dataset from France's past 10 games.

    \#\# 3 - Loading the Dataset

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{,} \PY{n}{test\PYZus{}X}\PY{p}{,} \PY{n}{test\PYZus{}Y} \PY{o}{=} \PY{n}{load\PYZus{}2D\PYZus{}dataset}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_7_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Each dot corresponds to a position on the football field where a
football player has hit the ball with his/her head after the French goal
keeper has shot the ball from the left side of the football field. - If
the dot is blue, it means the French player managed to hit the ball with
his/her head - If the dot is red, it means the other team's player hit
the ball with their head

\textbf{Your goal}: Use a deep learning model to find the positions on
the field where the goalkeeper should kick the ball.

    \textbf{Analysis of the dataset}: This dataset is a little noisy, but it
looks like a diagonal line separating the upper left half (blue) from
the lower right half (red) would work well.

You will first try a non-regularized model. Then you'll learn how to
regularize it and decide which model you will choose to solve the French
Football Corporation's problem.

    \#\# 4 - Non-Regularized Model

You will use the following neural network (already implemented for you
below). This model can be used: - in \emph{regularization mode} -- by
setting the \texttt{lambd} input to a non-zero value. We use
``\texttt{lambd}'' instead of ``\texttt{lambda}'' because
``\texttt{lambda}'' is a reserved keyword in Python. - in \emph{dropout
mode} -- by setting the \texttt{keep\_prob} to a value less than one

You will first try the model without any regularization. Then, you will
implement: - \emph{L2 regularization} -- functions:
``\texttt{compute\_cost\_with\_regularization()}'' and
``\texttt{backward\_propagation\_with\_regularization()}'' -
\emph{Dropout} -- functions:
``\texttt{forward\_propagation\_with\_dropout()}'' and
``\texttt{backward\_propagation\_with\_dropout()}''

In each part, you will run this model with the correct inputs so that it
calls the functions you've implemented. Take a look at the code below to
familiarize yourself with the model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{model}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{num\PYZus{}iterations} \PY{o}{=} \PY{l+m+mi}{30000}\PY{p}{,} \PY{n}{print\PYZus{}cost} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{lambd} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{keep\PYZus{}prob} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implements a three\PYZhy{}layer neural network: LINEAR\PYZhy{}\PYZgt{}RELU\PYZhy{}\PYZgt{}LINEAR\PYZhy{}\PYZgt{}RELU\PYZhy{}\PYZgt{}LINEAR\PYZhy{}\PYZgt{}SIGMOID.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    X \PYZhy{}\PYZhy{} input data, of shape (input size, number of examples)}
\PY{l+s+sd}{    Y \PYZhy{}\PYZhy{} true \PYZdq{}label\PYZdq{} vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)}
\PY{l+s+sd}{    learning\PYZus{}rate \PYZhy{}\PYZhy{} learning rate of the optimization}
\PY{l+s+sd}{    num\PYZus{}iterations \PYZhy{}\PYZhy{} number of iterations of the optimization loop}
\PY{l+s+sd}{    print\PYZus{}cost \PYZhy{}\PYZhy{} If True, print the cost every 10000 iterations}
\PY{l+s+sd}{    lambd \PYZhy{}\PYZhy{} regularization hyperparameter, scalar}
\PY{l+s+sd}{    keep\PYZus{}prob \PYZhy{} probability of keeping a neuron active during drop\PYZhy{}out, scalar.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} parameters learned by the model. They can then be used to predict.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
        
    \PY{n}{grads} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
    \PY{n}{costs} \PY{o}{=} \PY{p}{[}\PY{p}{]}                            \PY{c+c1}{\PYZsh{} to keep track of the cost}
    \PY{n}{m} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}                        \PY{c+c1}{\PYZsh{} number of examples}
    \PY{n}{layers\PYZus{}dims} \PY{o}{=} \PY{p}{[}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Initialize parameters dictionary.}
    \PY{n}{parameters} \PY{o}{=} \PY{n}{initialize\PYZus{}parameters}\PY{p}{(}\PY{n}{layers\PYZus{}dims}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Loop (gradient descent)}

    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{num\PYZus{}iterations}\PY{p}{)}\PY{p}{:}

        \PY{c+c1}{\PYZsh{} Forward propagation: LINEAR \PYZhy{}\PYZgt{} RELU \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} RELU \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} SIGMOID.}
        \PY{k}{if} \PY{n}{keep\PYZus{}prob} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{a3}\PY{p}{,} \PY{n}{cache} \PY{o}{=} \PY{n}{forward\PYZus{}propagation}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
        \PY{k}{elif} \PY{n}{keep\PYZus{}prob} \PY{o}{\PYZlt{}} \PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{a3}\PY{p}{,} \PY{n}{cache} \PY{o}{=} \PY{n}{forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{keep\PYZus{}prob}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Cost function}
        \PY{k}{if} \PY{n}{lambd} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{a3}\PY{p}{,} \PY{n}{Y}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost\PYZus{}with\PYZus{}regularization}\PY{p}{(}\PY{n}{a3}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{lambd}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{} Backward propagation.}
        \PY{k}{assert} \PY{p}{(}\PY{n}{lambd} \PY{o}{==} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{keep\PYZus{}prob} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{)}   \PY{c+c1}{\PYZsh{} it is possible to use both L2 regularization and dropout, }
                                                \PY{c+c1}{\PYZsh{} but this assignment will only explore one at a time}
        \PY{k}{if} \PY{n}{lambd} \PY{o}{==} \PY{l+m+mi}{0} \PY{o+ow}{and} \PY{n}{keep\PYZus{}prob} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{grads} \PY{o}{=} \PY{n}{backward\PYZus{}propagation}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{cache}\PY{p}{)}
        \PY{k}{elif} \PY{n}{lambd} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{grads} \PY{o}{=} \PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}regularization}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{cache}\PY{p}{,} \PY{n}{lambd}\PY{p}{)}
        \PY{k}{elif} \PY{n}{keep\PYZus{}prob} \PY{o}{\PYZlt{}} \PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{grads} \PY{o}{=} \PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{cache}\PY{p}{,} \PY{n}{keep\PYZus{}prob}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Update parameters.}
        \PY{n}{parameters} \PY{o}{=} \PY{n}{update\PYZus{}parameters}\PY{p}{(}\PY{n}{parameters}\PY{p}{,} \PY{n}{grads}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Print the loss every 10000 iterations}
        \PY{k}{if} \PY{n}{print\PYZus{}cost} \PY{o+ow}{and} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10000} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cost after iteration }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{cost}\PY{p}{)}\PY{p}{)}
        \PY{k}{if} \PY{n}{print\PYZus{}cost} \PY{o+ow}{and} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{1000} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{costs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cost}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} plot the cost}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{costs}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cost}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iterations (x1,000)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Learning rate =}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{parameters}
\end{Verbatim}
\end{tcolorbox}

    Let's train the model without any regularization, and observe the
accuracy on the train/test sets.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{parameters} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{On the training set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{predictions\PYZus{}train} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{On the test set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{predictions\PYZus{}test} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}X}\PY{p}{,} \PY{n}{test\PYZus{}Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cost after iteration 0: 0.6557412523481002
Cost after iteration 10000: 0.16329987525724204
Cost after iteration 20000: 0.13851642423234922
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
On the training set:
Accuracy: 0.9478672985781991
On the test set:
Accuracy: 0.915
    \end{Verbatim}

    The train accuracy is 94.8\% while the test accuracy is 91.5\%. This is
the \textbf{baseline model} (you will observe the impact of
regularization on this model). Run the following code to plot the
decision boundary of your model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Model without regularization}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
\PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.75}\PY{p}{,}\PY{l+m+mf}{0.40}\PY{p}{]}\PY{p}{)}
\PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.75}\PY{p}{,}\PY{l+m+mf}{0.65}\PY{p}{]}\PY{p}{)}
\PY{n}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{predict\PYZus{}dec}\PY{p}{(}\PY{n}{parameters}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The non-regularized model is obviously overfitting the training set. It
is fitting the noisy points! Lets now look at two techniques to reduce
overfitting.

    \#\# 5 - L2 Regularization

The standard way to avoid overfitting is called \textbf{L2
regularization}. It consists of appropriately modifying your cost
function, from:
\[J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}\]
To:
\[J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}\]

Let's modify your cost and observe the consequences.

\#\#\# Exercise 1 - compute\_cost\_with\_regularization Implement
\texttt{compute\_cost\_with\_regularization()} which computes the cost
given by formula (2). To calculate
\(\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}\) , use :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(np.square(Wl))}
\end{Highlighting}
\end{Shaded}

Note that you have to do this for \(W^{[1]}\), \(W^{[2]}\) and
\(W^{[3]}\), then sum the three terms and multiply by \$ \frac{1}{m}
\frac{\lambda}{2} \$.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}cost\PYZus{}with\PYZus{}regularization}

\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost\PYZus{}with\PYZus{}regularization}\PY{p}{(}\PY{n}{A3}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{lambd}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implement the cost function with L2 regularization. See formula (2) above.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    A3 \PYZhy{}\PYZhy{} post\PYZhy{}activation, output of forward propagation, of shape (output size, number of examples)}
\PY{l+s+sd}{    Y \PYZhy{}\PYZhy{} \PYZdq{}true\PYZdq{} labels vector, of shape (output size, number of examples)}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} python dictionary containing parameters of the model}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    cost \PYZhy{} value of the regularized loss function (formula (2))}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{m} \PY{o}{=} \PY{n}{Y}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{W1} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{W2} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{W3} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    
    \PY{n}{cross\PYZus{}entropy\PYZus{}cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{A3}\PY{p}{,} \PY{n}{Y}\PY{p}{)} \PY{c+c1}{\PYZsh{} This gives you the cross\PYZhy{}entropy part of the cost}
    
    \PY{c+c1}{\PYZsh{}(≈ 1 lines of code)}
    \PY{c+c1}{\PYZsh{} L2\PYZus{}regularization\PYZus{}cost = }
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{L2\PYZus{}regularization\PYZus{}cost} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{W1}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{W2}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{W3}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{lambd}\PY{o}{/}\PY{l+m+mi}{2}\PY{o}{/}\PY{n}{m}
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    
    \PY{n}{cost} \PY{o}{=} \PY{n}{cross\PYZus{}entropy\PYZus{}cost} \PY{o}{+} \PY{n}{L2\PYZus{}regularization\PYZus{}cost}
    
    \PY{k}{return} \PY{n}{cost}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{A3}\PY{p}{,} \PY{n}{t\PYZus{}Y}\PY{p}{,} \PY{n}{parameters} \PY{o}{=} \PY{n}{compute\PYZus{}cost\PYZus{}with\PYZus{}regularization\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}
\PY{n}{cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost\PYZus{}with\PYZus{}regularization}\PY{p}{(}\PY{n}{A3}\PY{p}{,} \PY{n}{t\PYZus{}Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{lambd}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cost = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{cost}\PY{p}{)}\PY{p}{)}

\PY{n}{compute\PYZus{}cost\PYZus{}with\PYZus{}regularization\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}cost\PYZus{}with\PYZus{}regularization}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
cost = 1.7864859451590758
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    Of course, because you changed the cost, you have to change backward
propagation as well! All the gradients have to be computed with respect
to this new cost.

\#\#\# Exercise 2 - backward\_propagation\_with\_regularization
Implement the changes needed in backward propagation to take into
account regularization. The changes only concern dW1, dW2 and dW3. For
each, you have to add the regularization term's gradient
(\(\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m} W^2) = \frac{\lambda}{m} W\)).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: backward\PYZus{}propagation\PYZus{}with\PYZus{}regularization}

\PY{k}{def} \PY{n+nf}{backward\PYZus{}propagation\PYZus{}with\PYZus{}regularization}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{cache}\PY{p}{,} \PY{n}{lambd}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implements the backward propagation of our baseline model to which we added an L2 regularization.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    X \PYZhy{}\PYZhy{} input dataset, of shape (input size, number of examples)}
\PY{l+s+sd}{    Y \PYZhy{}\PYZhy{} \PYZdq{}true\PYZdq{} labels vector, of shape (output size, number of examples)}
\PY{l+s+sd}{    cache \PYZhy{}\PYZhy{} cache output from forward\PYZus{}propagation()}
\PY{l+s+sd}{    lambd \PYZhy{}\PYZhy{} regularization hyperparameter, scalar}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    gradients \PYZhy{}\PYZhy{} A dictionary with the gradients with respect to each parameter, activation and pre\PYZhy{}activation variables}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{m} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{p}{(}\PY{n}{Z1}\PY{p}{,} \PY{n}{A1}\PY{p}{,} \PY{n}{W1}\PY{p}{,} \PY{n}{b1}\PY{p}{,} \PY{n}{Z2}\PY{p}{,} \PY{n}{A2}\PY{p}{,} \PY{n}{W2}\PY{p}{,} \PY{n}{b2}\PY{p}{,} \PY{n}{Z3}\PY{p}{,} \PY{n}{A3}\PY{p}{,} \PY{n}{W3}\PY{p}{,} \PY{n}{b3}\PY{p}{)} \PY{o}{=} \PY{n}{cache}
    
    \PY{n}{dZ3} \PY{o}{=} \PY{n}{A3} \PY{o}{\PYZhy{}} \PY{n}{Y}
    \PY{c+c1}{\PYZsh{}(≈ 1 lines of code)}
    \PY{n}{dW3} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dZ3}\PY{p}{,} \PY{n}{A2}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{o}{+} \PY{n}{lambd}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{W3}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{db3} \PY{o}{=} \PY{l+m+mf}{1.} \PY{o}{/} \PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dZ3}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    
    \PY{n}{dA2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W3}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{dZ3}\PY{p}{)}
    \PY{n}{dZ2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{multiply}\PY{p}{(}\PY{n}{dA2}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{int64}\PY{p}{(}\PY{n}{A2} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}(≈ 1 lines of code)}
    \PY{n}{dW2} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dZ2}\PY{p}{,} \PY{n}{A1}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{o}{+} \PY{n}{lambd}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{W2}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{db2} \PY{o}{=} \PY{l+m+mf}{1.} \PY{o}{/} \PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dZ2}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    
    \PY{n}{dA1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W2}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{dZ2}\PY{p}{)}
    \PY{n}{dZ1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{multiply}\PY{p}{(}\PY{n}{dA1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{int64}\PY{p}{(}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}(≈ 1 lines of code)}
    \PY{n}{dW1} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dZ1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{o}{+} \PY{n}{lambd}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{W1}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{db1} \PY{o}{=} \PY{l+m+mf}{1.} \PY{o}{/} \PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dZ1}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    
    \PY{n}{gradients} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dZ3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dZ3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dW3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{db3}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dA2}\PY{p}{,}
                 \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dZ2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dZ2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dW2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{db2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dA1}\PY{p}{,} 
                 \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dZ1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dZ1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dW1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{db1}\PY{p}{\PYZcb{}}
    
    \PY{k}{return} \PY{n}{gradients}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{t\PYZus{}Y}\PY{p}{,} \PY{n}{cache} \PY{o}{=} \PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}regularization\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}

\PY{n}{grads} \PY{o}{=} \PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}regularization}\PY{p}{(}\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{t\PYZus{}Y}\PY{p}{,} \PY{n}{cache}\PY{p}{,} \PY{n}{lambd} \PY{o}{=} \PY{l+m+mf}{0.7}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW1 = }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW2 = }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW3 = }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{grads}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}regularization\PYZus{}test}\PY{p}{(}\PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}regularization}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dW1 =
[[-0.25604646  0.12298827 -0.28297129]
 [-0.17706303  0.34536094 -0.4410571 ]]
dW2 =
[[ 0.79276486  0.85133918]
 [-0.0957219  -0.01720463]
 [-0.13100772 -0.03750433]]
dW3 =
[[-1.77691347 -0.11832879 -0.09397446]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    Let's now run the model with L2 regularization \((\lambda = 0.7)\). The
\texttt{model()} function will call: -
\texttt{compute\_cost\_with\_regularization} instead of
\texttt{compute\_cost} -
\texttt{backward\_propagation\_with\_regularization} instead of
\texttt{backward\_propagation}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{parameters} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{,} \PY{n}{lambd} \PY{o}{=} \PY{l+m+mf}{0.7}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{On the train set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{predictions\PYZus{}train} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{On the test set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{predictions\PYZus{}test} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}X}\PY{p}{,} \PY{n}{test\PYZus{}Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cost after iteration 0: 0.6974484493131264
Cost after iteration 10000: 0.2684918873282238
Cost after iteration 20000: 0.26809163371273004
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
On the train set:
Accuracy: 0.9383886255924171
On the test set:
Accuracy: 0.93
    \end{Verbatim}

    Congrats, the test set accuracy increased to 93\%. You have saved the
French football team!

You are not overfitting the training data anymore. Let's plot the
decision boundary.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Model with L2\PYZhy{}regularization}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
\PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.75}\PY{p}{,}\PY{l+m+mf}{0.40}\PY{p}{]}\PY{p}{)}
\PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.75}\PY{p}{,}\PY{l+m+mf}{0.65}\PY{p}{]}\PY{p}{)}
\PY{n}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{predict\PYZus{}dec}\PY{p}{(}\PY{n}{parameters}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Observations}: - The value of \(\lambda\) is a hyperparameter
that you can tune using a dev set. - L2 regularization makes your
decision boundary smoother. If \(\lambda\) is too large, it is also
possible to ``oversmooth'', resulting in a model with high bias.

\textbf{What is L2-regularization actually doing?}:

L2-regularization relies on the assumption that a model with small
weights is simpler than a model with large weights. Thus, by penalizing
the square values of the weights in the cost function you drive all the
weights to smaller values. It becomes too costly for the cost to have
large weights! This leads to a smoother model in which the output
changes more slowly as the input changes.

\textbf{What you should remember:} the implications of L2-regularization
on: - The cost computation: - A regularization term is added to the
cost. - The backpropagation function: - There are extra terms in the
gradients with respect to weight matrices. - Weights end up smaller
(``weight decay''): - Weights are pushed to smaller values.

    \#\# 6 - Dropout

Finally, \textbf{dropout} is a widely used regularization technique that
is specific to deep learning. \textbf{It randomly shuts down some
neurons in each iteration.} Watch these two videos to see what this
means!

Figure 2 : Drop-out on the second hidden layer. At each iteration, you
shut down (= set to zero) each neuron of a layer with probability
\(1 - keep\_prob\) or keep it with probability \(keep\_prob\) (50\%
here). The dropped neurons don't contribute to the training in both the
forward and backward propagations of the iteration.

Figure 3: Drop-out on the first and third hidden layers. \(1^{st}\)
layer: we shut down on average 40\% of the neurons. \(3^{rd}\) layer: we
shut down on average 20\% of the neurons.

When you shut some neurons down, you actually modify your model. The
idea behind drop-out is that at each iteration, you train a different
model that uses only a subset of your neurons. With dropout, your
neurons thus become less sensitive to the activation of one other
specific neuron, because that other neuron might be shut down at any
time.

\#\#\# 6.1 - Forward Propagation with Dropout

\#\#\# Exercise 3 - forward\_propagation\_with\_dropout

Implement the forward propagation with dropout. You are using a 3 layer
neural network, and will add dropout to the first and second hidden
layers. We will not apply dropout to the input layer or output layer.

\textbf{Instructions}: You would like to shut down some neurons in the
first and second layers. To do that, you are going to carry out 4 Steps:
1. In lecture, we dicussed creating a variable \(d^{[1]}\) with the same
shape as \(a^{[1]}\) using \texttt{np.random.rand()} to randomly get
numbers between 0 and 1. Here, you will use a vectorized implementation,
so create a random matrix \$D\^{}\{{[}1{]}\} = {[}d\^{}\{\url{1}\}
d\^{}\{\href{2}{1}\} \ldots{} d\^{}\{\href{m}{1}\}{]} \$ of the same
dimension as \(A^{[1]}\). 2. Set each entry of \(D^{[1]}\) to be 1 with
probability (\texttt{keep\_prob}), and 0 otherwise.

\textbf{Hint:} Let's say that keep\_prob = 0.8, which means that we want
to keep about 80\% of the neurons and drop out about 20\% of them. We
want to generate a vector that has 1's and 0's, where about 80\% of them
are 1 and about 20\% are 0. This python statement:\\
\texttt{X\ =\ (X\ \textless{}\ keep\_prob).astype(int)}

is conceptually the same as this if-else statement (for the simple case
of a one-dimensional array) :

\begin{verbatim}
for i,v in enumerate(x):
    if v < keep_prob:
        x[i] = 1
    else: # v >= keep_prob
        x[i] = 0
\end{verbatim}

Note that the \texttt{X\ =\ (X\ \textless{}\ keep\_prob).astype(int)}
works with multi-dimensional arrays, and the resulting output preserves
the dimensions of the input array.

Also note that without using \texttt{.astype(int)}, the result is an
array of booleans \texttt{True} and \texttt{False}, which Python
automatically converts to 1 and 0 if we multiply it with numbers.
(However, it's better practice to convert data into the data type that
we intend, so try using \texttt{.astype(int)}.)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Set \(A^{[1]}\) to \(A^{[1]} * D^{[1]}\). (You are shutting down some
  neurons). You can think of \(D^{[1]}\) as a mask, so that when it is
  multiplied with another matrix, it shuts down some of the values.
\item
  Divide \(A^{[1]}\) by \texttt{keep\_prob}. By doing this you are
  assuring that the result of the cost will still have the same expected
  value as without drop-out. (This technique is also called inverted
  dropout.)
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}

\PY{k}{def} \PY{n+nf}{forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{keep\PYZus{}prob} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implements the forward propagation: LINEAR \PYZhy{}\PYZgt{} RELU + DROPOUT \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} RELU + DROPOUT \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} SIGMOID.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    X \PYZhy{}\PYZhy{} input dataset, of shape (2, number of examples)}
\PY{l+s+sd}{    parameters \PYZhy{}\PYZhy{} python dictionary containing your parameters \PYZdq{}W1\PYZdq{}, \PYZdq{}b1\PYZdq{}, \PYZdq{}W2\PYZdq{}, \PYZdq{}b2\PYZdq{}, \PYZdq{}W3\PYZdq{}, \PYZdq{}b3\PYZdq{}:}
\PY{l+s+sd}{                    W1 \PYZhy{}\PYZhy{} weight matrix of shape (20, 2)}
\PY{l+s+sd}{                    b1 \PYZhy{}\PYZhy{} bias vector of shape (20, 1)}
\PY{l+s+sd}{                    W2 \PYZhy{}\PYZhy{} weight matrix of shape (3, 20)}
\PY{l+s+sd}{                    b2 \PYZhy{}\PYZhy{} bias vector of shape (3, 1)}
\PY{l+s+sd}{                    W3 \PYZhy{}\PYZhy{} weight matrix of shape (1, 3)}
\PY{l+s+sd}{                    b3 \PYZhy{}\PYZhy{} bias vector of shape (1, 1)}
\PY{l+s+sd}{    keep\PYZus{}prob \PYZhy{} probability of keeping a neuron active during drop\PYZhy{}out, scalar}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    A3 \PYZhy{}\PYZhy{} last activation value, output of the forward propagation, of shape (1,1)}
\PY{l+s+sd}{    cache \PYZhy{}\PYZhy{} tuple, information stored for computing the backward propagation}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} retrieve parameters}
    \PY{n}{W1} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{b1} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{W2} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{b2} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{W3} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{b3} \PY{o}{=} \PY{n}{parameters}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} LINEAR \PYZhy{}\PYZgt{} RELU \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} RELU \PYZhy{}\PYZgt{} LINEAR \PYZhy{}\PYZgt{} SIGMOID}
    \PY{n}{Z1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W1}\PY{p}{,} \PY{n}{X}\PY{p}{)} \PY{o}{+} \PY{n}{b1}
    \PY{n}{A1} \PY{o}{=} \PY{n}{relu}\PY{p}{(}\PY{n}{Z1}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}(≈ 4 lines of code)         \PYZsh{} Steps 1\PYZhy{}4 below correspond to the Steps 1\PYZhy{}4 described above. }
    \PY{c+c1}{\PYZsh{} D1 =                                           \PYZsh{} Step 1: initialize matrix D1 = np.random.rand(..., ...)}
    \PY{c+c1}{\PYZsh{} D1 =                                           \PYZsh{} Step 2: convert entries of D1 to 0 or 1 (using keep\PYZus{}prob as the threshold)}
    \PY{c+c1}{\PYZsh{} A1 =                                           \PYZsh{} Step 3: shut down some neurons of A1}
    \PY{c+c1}{\PYZsh{} A1 =                                           \PYZsh{} Step 4: scale the value of neurons that haven\PYZsq{}t been shut down}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{D1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{o}{*}\PY{n}{A1}\PY{o}{.}\PY{n}{shape}\PY{p}{)}                                          
    \PY{n}{D1} \PY{o}{=} \PY{p}{(}\PY{n}{D1} \PY{o}{\PYZlt{}} \PY{n}{keep\PYZus{}prob}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}                                     
    \PY{n}{A1} \PY{o}{=} \PY{n}{A1} \PY{o}{*} \PY{n}{D1}                                        
    \PY{n}{A1} \PY{o}{=} \PY{n}{A1} \PY{o}{/} \PY{n}{keep\PYZus{}prob}                                         
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{Z2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W2}\PY{p}{,} \PY{n}{A1}\PY{p}{)} \PY{o}{+} \PY{n}{b2}
    \PY{n}{A2} \PY{o}{=} \PY{n}{relu}\PY{p}{(}\PY{n}{Z2}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}(≈ 4 lines of code)}
    \PY{c+c1}{\PYZsh{} D2 =                                           \PYZsh{} Step 1: initialize matrix D2 = np.random.rand(..., ...)}
    \PY{c+c1}{\PYZsh{} D2 =                                           \PYZsh{} Step 2: convert entries of D2 to 0 or 1 (using keep\PYZus{}prob as the threshold)}
    \PY{c+c1}{\PYZsh{} A2 =                                           \PYZsh{} Step 3: shut down some neurons of A2}
    \PY{c+c1}{\PYZsh{} A2 =                                           \PYZsh{} Step 4: scale the value of neurons that haven\PYZsq{}t been shut down}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{D2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{o}{*}\PY{n}{A2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}                                                 
    \PY{n}{D2} \PY{o}{=} \PY{p}{(}\PY{n}{D2} \PY{o}{\PYZlt{}} \PY{n}{keep\PYZus{}prob}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}                                          
    \PY{n}{A2} \PY{o}{=} \PY{n}{A2} \PY{o}{*} \PY{n}{D2}                                             
    \PY{n}{A2} \PY{o}{=} \PY{n}{A2} \PY{o}{/} \PY{n}{keep\PYZus{}prob}           
    
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{Z3} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W3}\PY{p}{,} \PY{n}{A2}\PY{p}{)} \PY{o}{+} \PY{n}{b3}
    \PY{n}{A3} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{Z3}\PY{p}{)}
    
    \PY{n}{cache} \PY{o}{=} \PY{p}{(}\PY{n}{Z1}\PY{p}{,} \PY{n}{D1}\PY{p}{,} \PY{n}{A1}\PY{p}{,} \PY{n}{W1}\PY{p}{,} \PY{n}{b1}\PY{p}{,} \PY{n}{Z2}\PY{p}{,} \PY{n}{D2}\PY{p}{,} \PY{n}{A2}\PY{p}{,} \PY{n}{W2}\PY{p}{,} \PY{n}{b2}\PY{p}{,} \PY{n}{Z3}\PY{p}{,} \PY{n}{A3}\PY{p}{,} \PY{n}{W3}\PY{p}{,} \PY{n}{b3}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{A3}\PY{p}{,} \PY{n}{cache}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{parameters} \PY{o}{=} \PY{n}{forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}

\PY{n}{A3}\PY{p}{,} \PY{n}{cache} \PY{o}{=} \PY{n}{forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{(}\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{keep\PYZus{}prob}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A3 = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{A3}\PY{p}{)}\PY{p}{)}

\PY{n}{forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout\PYZus{}test}\PY{p}{(}\PY{n}{forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
A3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    \#\#\# 6.2 - Backward Propagation with Dropout

\#\#\# Exercise 4 - backward\_propagation\_with\_dropout Implement the
backward propagation with dropout. As before, you are training a 3 layer
network. Add dropout to the first and second hidden layers, using the
masks \(D^{[1]}\) and \(D^{[2]}\) stored in the cache.

\textbf{Instruction}: Backpropagation with dropout is actually quite
easy. You will have to carry out 2 Steps: 1. You had previously shut
down some neurons during forward propagation, by applying a mask
\(D^{[1]}\) to \texttt{A1}. In backpropagation, you will have to shut
down the same neurons, by reapplying the same mask \(D^{[1]}\) to
\texttt{dA1}. 2. During forward propagation, you had divided \texttt{A1}
by \texttt{keep\_prob}. In backpropagation, you'll therefore have to
divide \texttt{dA1} by \texttt{keep\_prob} again (the calculus
interpretation is that if \(A^{[1]}\) is scaled by \texttt{keep\_prob},
then its derivative \(dA^{[1]}\) is also scaled by the same
\texttt{keep\_prob}).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: backward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}

\PY{k}{def} \PY{n+nf}{backward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{cache}\PY{p}{,} \PY{n}{keep\PYZus{}prob}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Implements the backward propagation of our baseline model to which we added dropout.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    X \PYZhy{}\PYZhy{} input dataset, of shape (2, number of examples)}
\PY{l+s+sd}{    Y \PYZhy{}\PYZhy{} \PYZdq{}true\PYZdq{} labels vector, of shape (output size, number of examples)}
\PY{l+s+sd}{    cache \PYZhy{}\PYZhy{} cache output from forward\PYZus{}propagation\PYZus{}with\PYZus{}dropout()}
\PY{l+s+sd}{    keep\PYZus{}prob \PYZhy{} probability of keeping a neuron active during drop\PYZhy{}out, scalar}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    gradients \PYZhy{}\PYZhy{} A dictionary with the gradients with respect to each parameter, activation and pre\PYZhy{}activation variables}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{m} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{p}{(}\PY{n}{Z1}\PY{p}{,} \PY{n}{D1}\PY{p}{,} \PY{n}{A1}\PY{p}{,} \PY{n}{W1}\PY{p}{,} \PY{n}{b1}\PY{p}{,} \PY{n}{Z2}\PY{p}{,} \PY{n}{D2}\PY{p}{,} \PY{n}{A2}\PY{p}{,} \PY{n}{W2}\PY{p}{,} \PY{n}{b2}\PY{p}{,} \PY{n}{Z3}\PY{p}{,} \PY{n}{A3}\PY{p}{,} \PY{n}{W3}\PY{p}{,} \PY{n}{b3}\PY{p}{)} \PY{o}{=} \PY{n}{cache}
    
    \PY{n}{dZ3} \PY{o}{=} \PY{n}{A3} \PY{o}{\PYZhy{}} \PY{n}{Y}
    \PY{n}{dW3} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dZ3}\PY{p}{,} \PY{n}{A2}\PY{o}{.}\PY{n}{T}\PY{p}{)}
    \PY{n}{db3} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dZ3}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{dA2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W3}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{dZ3}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
    \PY{c+c1}{\PYZsh{} dA2 =                \PYZsh{} Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation}
    \PY{c+c1}{\PYZsh{} dA2 =                \PYZsh{} Step 2: Scale the value of neurons that haven\PYZsq{}t been shut down}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{dA2} \PY{o}{=} \PY{n}{D2} \PY{o}{*} \PY{n}{dA2}
    \PY{n}{dA2} \PY{o}{=} \PY{n}{dA2} \PY{o}{/} \PY{n}{keep\PYZus{}prob}
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{dZ2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{multiply}\PY{p}{(}\PY{n}{dA2}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{int64}\PY{p}{(}\PY{n}{A2} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
    \PY{n}{dW2} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dZ2}\PY{p}{,} \PY{n}{A1}\PY{o}{.}\PY{n}{T}\PY{p}{)}
    \PY{n}{db2} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dZ2}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    
    \PY{n}{dA1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W2}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{dZ2}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}(≈ 2 lines of code)}
    \PY{c+c1}{\PYZsh{} dA1 =                \PYZsh{} Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation}
    \PY{c+c1}{\PYZsh{} dA1 =                \PYZsh{} Step 2: Scale the value of neurons that haven\PYZsq{}t been shut down}
    \PY{c+c1}{\PYZsh{} YOUR CODE STARTS HERE}
    \PY{n}{dA1} \PY{o}{=} \PY{n}{D1} \PY{o}{*} \PY{n}{dA1}
    \PY{n}{dA1} \PY{o}{=} \PY{n}{dA1} \PY{o}{/} \PY{n}{keep\PYZus{}prob}
    \PY{c+c1}{\PYZsh{} YOUR CODE ENDS HERE}
    \PY{n}{dZ1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{multiply}\PY{p}{(}\PY{n}{dA1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{int64}\PY{p}{(}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
    \PY{n}{dW1} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dZ1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{)}
    \PY{n}{db1} \PY{o}{=} \PY{l+m+mf}{1.}\PY{o}{/}\PY{n}{m} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{dZ1}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    
    \PY{n}{gradients} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dZ3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dZ3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dW3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{db3}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dA2}\PY{p}{,}
                 \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dZ2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dZ2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dW2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{db2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dA1}\PY{p}{,} 
                 \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dZ1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dZ1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dW1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{dW1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{db1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{db1}\PY{p}{\PYZcb{}}
    
    \PY{k}{return} \PY{n}{gradients}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{t\PYZus{}Y}\PY{p}{,} \PY{n}{cache} \PY{o}{=} \PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}dropout\PYZus{}test\PYZus{}case}\PY{p}{(}\PY{p}{)}

\PY{n}{gradients} \PY{o}{=} \PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{(}\PY{n}{t\PYZus{}X}\PY{p}{,} \PY{n}{t\PYZus{}Y}\PY{p}{,} \PY{n}{cache}\PY{p}{,} \PY{n}{keep\PYZus{}prob}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{)}

\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA1 = }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{gradients}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA2 = }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{gradients}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dA2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}dropout\PYZus{}test}\PY{p}{(}\PY{n}{backward\PYZus{}propagation\PYZus{}with\PYZus{}dropout}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dA1 =
[[ 0.36544439  0.         -0.00188233  0.         -0.17408748]
 [ 0.65515713  0.         -0.00337459  0.         -0.        ]]
dA2 =
[[ 0.58180856  0.         -0.00299679  0.         -0.27715731]
 [ 0.          0.53159854 -0.          0.53159854 -0.34089673]
 [ 0.          0.         -0.00292733  0.         -0.        ]]
\textcolor{ansi-green-intense}{ All tests passed.}
    \end{Verbatim}

    Let's now run the model with dropout (\texttt{keep\_prob\ =\ 0.86}). It
means at every iteration you shut down each neurons of layer 1 and 2
with 14\% probability. The function \texttt{model()} will now call: -
\texttt{forward\_propagation\_with\_dropout} instead of
\texttt{forward\_propagation}. -
\texttt{backward\_propagation\_with\_dropout} instead of
\texttt{backward\_propagation}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{parameters} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{,} \PY{n}{keep\PYZus{}prob} \PY{o}{=} \PY{l+m+mf}{0.86}\PY{p}{,} \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{)}

\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{On the train set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{predictions\PYZus{}train} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{On the test set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{predictions\PYZus{}test} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}X}\PY{p}{,} \PY{n}{test\PYZus{}Y}\PY{p}{,} \PY{n}{parameters}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cost after iteration 0: 0.6543912405149825
Cost after iteration 10000: 0.0610169865749056
Cost after iteration 20000: 0.060582435798513114
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
On the train set:
Accuracy: 0.9289099526066351
On the test set:
Accuracy: 0.95
    \end{Verbatim}

    Dropout works great! The test accuracy has increased again (to 95\%)!
Your model is not overfitting the training set and does a great job on
the test set. The French football team will be forever grateful to you!

Run the code below to plot the decision boundary.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Model with dropout}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
\PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.75}\PY{p}{,}\PY{l+m+mf}{0.40}\PY{p}{]}\PY{p}{)}
\PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.75}\PY{p}{,}\PY{l+m+mf}{0.65}\PY{p}{]}\PY{p}{)}
\PY{n}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{predict\PYZus{}dec}\PY{p}{(}\PY{n}{parameters}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}Y}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Note}: - A \textbf{common mistake} when using dropout is to use
it both in training and testing. You should use dropout (randomly
eliminate nodes) only in training. - Deep learning frameworks like
\href{https://www.tensorflow.org/api_docs/python/tf/nn/dropout}{tensorflow},
\href{http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html}{PaddlePaddle},
\href{https://keras.io/layers/core/\#dropout}{keras} or
\href{http://caffe.berkeleyvision.org/tutorial/layers/dropout.html}{caffe}
come with a dropout layer implementation. Don't stress - you will soon
learn some of these frameworks.

\textbf{What you should remember about dropout:} - Dropout is a
regularization technique. - You only use dropout during training. Don't
use dropout (randomly eliminate nodes) during test time. - Apply dropout
both during forward and backward propagation. - During training time,
divide each dropout layer by keep\_prob to keep the same expected value
for the activations. For example, if keep\_prob is 0.5, then we will on
average shut down half the nodes, so the output will be scaled by 0.5
since only the remaining half are contributing to the solution. Dividing
by 0.5 is equivalent to multiplying by 2. Hence, the output now has the
same expected value. You can check that this works even when keep\_prob
is other values than 0.5.

    \#\# 7 - Conclusions

    \textbf{Here are the results of our three models}:

model

train accuracy

test accuracy

\begin{verbatim}
<td>
    3-layer NN without regularization
    </td>
    <td>
    95%
    </td>
    <td>
    91.5%
    </td>
<tr>
    <td>
    3-layer NN with L2-regularization
    </td>
    <td>
    94%
    </td>
    <td>
    93%
    </td>
</tr>
<tr>
    <td>
    3-layer NN with dropout
    </td>
    <td>
    93%
    </td>
    <td>
    95%
    </td>
</tr>
\end{verbatim}

    Note that regularization hurts training set performance! This is because
it limits the ability of the network to overfit to the training set. But
since it ultimately gives better test accuracy, it is helping your
system.

    Congratulations for finishing this assignment! And also for
revolutionizing French football. :-)

    \textbf{What we want you to remember from this notebook}: -
Regularization will help you reduce overfitting. - Regularization will
drive your weights to lower values. - L2 regularization and Dropout are
two very effective regularization techniques.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
